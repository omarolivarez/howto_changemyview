---
title: "01 How To changemyview"
output: pdf_document
---

Steps to run before this
1. Download Google Sheet
2. Run through LIWC and save as xlsx
3. Clean up first 3 col names
4. Make sure file keeps same name (liwc_results_changemyview.xlsx) and filepath. 

Then you can start from here

```{r}
# READ IN LIBRARIES
library(data.table)
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(tidyverse)
library(stringr) # used to replace strings " of "
library(ggpubr) # used to stitch multiple graphs together
library(readxl) # to read in excel
library(corrplot) # to draw correlogram
library(corrr) # to focus correlation specifically on changedview
library(lmtest) # for likelihood test for glm
library(tree)
library(glmnet)
#library(randomForest)
#library (gbm)
```


```{r}
df_file_path <- "/Users/oolivarez/Desktop/workspaces/howto_changemyview/data/liwc_results_changemyview.xlsx"

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# and read in main, LIWCed data
df <- read_excel(df_file_path)

View(head(df, 30))
dim(df)
#head(df)
```

Need to convert changedview from character to numeric in order to use correlations and other calculations. 
```{r}
df$changedview <- as.numeric(df$changedview)
```


Let's select only relevant vars and see their correlation coefficient
```{r}
mini_df <- df %>%
  select(-c(area, text, post))

corr_df <- mini_df %>% correlate() %>% focus(changedview) 

corr_df <- corr_df %>% 
  arrange(desc(corr_df$changedview))
View(corr_df)
```

Words highly positively correlated with changing view (above 0.1 corr):
compare
WPS (is close)

Words highly negatively correlated with changing view (below -0.2 corr):
QMark
ingest
certain (is close)

## More robust analyses
Let's visualize the distribution of the two variables at the extremes
```{r}
ggplot(df, aes(x = work, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))

ggplot(df, aes(x = QMark, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))
```

Now let's run a logistic regression using the above variables
NOTE: this block isn't so useful anymore. 
```{r}
logit_view1 <- glm(changedview ~ work, family = binomial,
                   data = df)
summary(logit_view1)

```





Use this section to compare models to decide which vars to use

```{r}

glm1 <- glm(changedview ~ work + WC + WPS + compare + negate + 
              QMark + certain, 
            family = binomial, 
            data = df)
#summary(glm1)

glm2 <- glm(changedview ~ work + WC + WPS + compare + negate + 
              QMark + certain + interrog,
            family = binomial,
            data = df)
#summary(logit_view2)

lrtest(glm1, glm2)
```





```{r}
ggplot(df, aes(x=work, y= changedview)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", fullrange=TRUE, 
              method.args = list(family=binomial)) 
```


CV'd accuracy here for the entire df is 54.13%.

NOTE: USE THIS MODEL BELOW FOR YOUR WORK:
Now do crossvalidation and only use select columns. 

```{r}
# train the model on training set
set.seed(222)
library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
df_g$changedview <- as.factor(df_g$changedview)
model <- train(changedview ~ work + WC + WPS + compare + negate + QMark + certain,
               data = df_g,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```

With an accuracy of 62.58%, this is the best model to use.


Note: Don't use this one. 
Creating a GLM from training data (not crossvaldiated) and testing it:

```{r}
set.seed(333)
test_obs              <- round(0.06 * nrow(df))
train_obs             <- nrow(df) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df), replace = FALSE)
test_data             <- df[which(test_train_vec == "test"),]
train_data            <- df[which(test_train_vec == "train"),]
train_data <- train_data %>% select(-c(text, area))

# create the model
glm_model      <- glm(changedview ~  compare + affiliation + QMark + leisure + certain,
                     family = binomial,
                     data = train_data)

# make the predictions
logistic_predict      <- predict(glm_model,
                                 test_data,
                                 type = "response")

# show the results
class_predictions     <- as.numeric(logistic_predict > 0.5)
logistic_accuracy     <- mean(class_predictions == test_data$changedview)
print(logistic_accuracy)
```
Accuracy^

Note: Don't use this one. 
Now trying that crossvalidated and using all columns:

```{r}
library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
df_g$changedview <- as.factor(df_g$changedview)
#df_g <- df_g %>% select(-post)
# train the model on training set
model <- train(changedview ~ .,
               data = df_g,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```

## LASSO Logistic Regression

Creating a LASSO logistic regression and testing it

```{r}
df_g <- df %>% select(-c(text, area, post))
#head(df_g)
set.seed(333)
test_obs              <- round(0.03 * nrow(df_g))
train_obs             <- nrow(df_g) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df_g), replace = FALSE)
test_data             <- df_g[which(test_train_vec == "test"),]
train_data            <- df_g[which(test_train_vec == "train"),]
#train_data <- train_data %>% select(-c(text, area))
train_data_x <- df_g[ , -1]
train_data_x <- as.matrix(sapply(train_data_x, as.numeric))
train_data_y <- df_g[ , 1]
train_data_y <- as.matrix(sapply(train_data_y, as.numeric))


# create the model
glmmod <- cv.glmnet(train_data_x, train_data_y, alpha=1, 
                    nfolds = 10,
                    family="binomial",
                    type.measure = 'class')
summary(glmmod)
print("END OF MODEL SUMMARY")

test_data_x <- test_data[ , -1]
test_data_x <- as.matrix(sapply(test_data_x, as.numeric))
test_data_y <- test_data[ , 1]


# make the predictions
#logistic_predict      <- predict(glmmod,
#                                 test_data_x,
#                                 type = "response")
# show the results
#class_predictions     <- as.numeric(logistic_predict > 0.5)
#logistic_accuracy     <- mean(class_predictions == test_data$changedview)
#print(logistic_accuracy)

# let's take a look at the accuracy of this model
print("MODEL ACCURACY")
print(round(100 * mean(glmmod$cvm), 3))
```

With accuracy of 48.79%, this doesn't look like a very accurate model, I'm not going to pursue it further.


## RANDOM FORESTS

Playing around with random forests
```{r}
set.seed(222)
high_sales          <- as.factor(df$changedview)
carseat_data        <- data.frame(df_g, high_sales)
carseat_data = carseat_data[, -1]
train               <- sample(seq(nrow(carseat_data)),
                              round(nrow(carseat_data) * 0.85))
train               <- sort(train)
test                <- which(!(seq(nrow(carseat_data)) %in% train))
carseats_tree       <- tree(high_sales ~., 
                            data = carseat_data[train,])

plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
```

Now let's get the error info:

```{r}
error_rate_func   <- function(predictions, true_vals) {
  error_rate      <- mean(as.numeric(predictions != true_vals))
  return(error_rate)
}
print("~")
deep_tree_preds   <- predict(carseats_tree,
                             carseat_data[test, ],
                             type = "class")
print("ERROR RATE IS:")
error_rate_func(deep_tree_preds, carseat_data[test,"high_sales"])
print("~")
summary(carseats_tree)
```


Pruning:

```{r}
set.seed(20)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN=prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree

## Size tells us the number of terminal nodes on each
## of the trees considered; dev gives us the CV errors;
## k gives us the cost-complexity parameter.
## We can plot the error as a function of size and k

par(mfrow =c(1,2))

plot(cv_carseats_tree$size ,cv_carseats_tree$dev ,type="b")
plot(cv_carseats_tree$k ,cv_carseats_tree$dev ,type="b")

opt_indx          <- which.min(cv_carseats_tree$dev)
opt_size          <- cv_carseats_tree$size[opt_indx]

print(opt_size)

## Now we can prune the tree using prune.misclass()

pruned_carseats_tree  <- prune.misclass(carseats_tree,
                                        best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
print("ERROR RATE FUNCTION")
error_rate_func(predict(pruned_carseats_tree, carseat_data[test, ], 
                        type = "class")
                , carseat_data[test,"high_sales"])
```





```{r}
```
