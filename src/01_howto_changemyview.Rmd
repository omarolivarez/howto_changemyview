---
title: "01 How To changemyview"
output: pdf_document
---

Steps to run before this
1. Download Google Sheet
2. Run through LIWC and save as xlsx
3. Clean up first 3 col names
4. Make sure file keeps same name (liwc_results_changemyview.xlsx) and filepath. 

Then you can start from here

```{r}
# READ IN LIBRARIES
library(data.table)
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(tidyverse)
library(stringr) # used to replace strings " of "
library(ggpubr) # used to stitch multiple graphs together
library(readxl) # to read in excel
library(corrplot) # to draw correlogram
library(corrr) # to focus correlation specifically on changedview
library(lmtest) # for likelihood test for glm
library(tree)
library(glmnet)
library(caret)
library(coefplot) # to extract the coefficients from the GLM LASSO
#library(randomForest)
#library (gbm)
```


```{r}
df_file_path <- "/Users/oolivarez/Desktop/workspaces/howto_changemyview/data/liwc_results_changemyview.xlsx"

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# and read in main, LIWCed data
df <- read_excel(df_file_path)

#View(head(df, 30))
dim(df)
#head(df)
```

```{r}
#Need to convert changedview from character to numeric in order to use correlations and other calculations. 
df$changedview <- as.numeric(df$changedview)

#Let's select only relevant vars and see their correlation coefficient
mini_df <- df %>% select(changedview:OtherP) %>%
  select(-c(area, text, author_text, post, author_post))
mini_df <- mini_df %>% mutate_if(is.character,as.numeric)

#mini_df = lapply(mini_df,as.numeric)
mini_df <- as.data.frame(mini_df)
#str(mini_df)
#print(sum(is.na(mini_df$changedview)))
# corr_df <- mini_df %>% correlate() %>% focus(mini_df$changedview)
# corr_df <- corr_df %>%
#  arrange(desc(corr_df$changedview))
# View(corr_df)

small <- cor(mini_df[-1], mini_df$changedview)
small <- as.data.frame(small)
corr_df <- small %>%
  arrange(desc(small$V1))
View(corr_df)
```



Experimentation with simpler table
THIS IS ONE OF TH3E MODELS TO USE
NOTE:E TRY A LINEAR PROBABILITY MODEL LATER

```{r}
sh_df <- mini_df
sh_df$changedview <- as.numeric(sh_df$changedview)

# Principal Components work:
post_df <- df %>% select(article, posemo, negemo, negate, anger) # Dic <- I'm going to remove Dic for now because I don't know what it means
# I'm also removing: , you, Exclam
post.pca <- prcomp(post_df, center = TRUE,scale. = TRUE)
summary(post.pca)
sh_df_new <- cbind(sh_df,post.pca$x[,1:3]) # this adds the first component to comments df

# add the three components to the df
names(sh_df_new)[length(names(sh_df_new)) - 2]<-"PC1" 
names(sh_df_new)[length(names(sh_df_new)) - 1]<-"PC2" 
names(sh_df_new)[length(names(sh_df_new)) ]<-"PC3" 
sh_df_new <- sh_df_new %>% mutate_if(is.character,as.numeric)

#View(sh_df_new)
```
With just 3 componets we get 90% coverage. 

Now test which variables matter:
```{r}
# NOTE: for some reason changedview had 1 added to it, now we'll subtract one and figure out what happened later. 
#sh_df_new$changedview <- sh_df_new$changedview - 1

gm1 <- glm(changedview ~ WC_c + WPS_c + Parenth_c + SemiC_c +
             QMark_c + relig_c + certain_c + Period_c,
           family = binomial,
           data = sh_df_new)
#summary(gm1)

gm2 <- glm(changedview ~ WC_c + WPS_c + Parenth_c + SemiC_c + 
             QMark_c + relig_c + certain_c + Period_c + anx_c,
          family = binomial,
           data = sh_df_new)
#summary(logit_view2)
lrtest(gm1, gm2)
#nested models - check against this guidance? https://r4ds.had.co.nz/many-models.html
```

Now let's test the model accuracy using GLM and Cross-Validation:
```{r}
train_control <- trainControl(method = "cv", number = 10)
sdn <- sh_df_new
sdn$changedview <- as.factor(sdn$changedview)
model_3 <- train(changedview ~ WC_c + Parenth_c + # removed WPS_c and SemiC_c because they were not stat sig and accuracy has now improved
                   QMark_c + relig_c + certain_c + Period_c + 
                   PC1 + PC2,
                data = sdn,
                trControl = train_control,
                method = "glm",
                family=binomial())

summary(model_3)
print("MODEL ACCURACY")
#print(model$MAE)
print(model_3)
```


NOTE TO SELF: MAKE APPENDIX TO COMPARE FOR ORIGINAL 5 ATTRIBUTES

64.32% accuracy. Vars I'm using prior to the addition of the PC: WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + certain_c + reward_c

Okay, I've tried interacting each coefficient with PC1 and the ones that were significant were when Qmark_c + certain_c + reward_c were interacted with PC1. The classification was lower (63%) but maybe that doesn't matter in this case of inference? Should I leave the interactions in even if they aren't statistically significant? (that's a question for Oluchi)

I've tried just controlling for PC1 (by just adding it into the model.) That classification was 63%.

NEXT: use PC1 as an IV for everything
Even if it has lower accuracy, I should make sure to leave in the interaction/fixed effects/IV/whatever I choose, because that controls for things that I want to control for. 

I think in the end, I'm going to go with: just adding in PC1 + PC2 + PC3. Including interactions would mean: "I assume that the agreeableness of the post affects how someone would respond to it (I think). And instead, I want to say, ONCE I CONTROL for someone's agreeableness, what are the LIWC vars that would change people's minds. I HAVE TRIED THIS OUT, NEXT STEPS IS: add in PC1:PC3 into the model first, then do the lrtest on all the vars again to see if they change once we control for these 3 components. 
Updates:
These vars WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + certain_c  + reward_c + PC1 have accuracy of 64.65%. 

Most recent model: article + posemo + negemo + Exclam + negate + WC_c + WPS_c + adj_c + work_c + negate_c  +
                   Qmark_c + certain_c + reward_c
Which has 64.10% accuracy

Now this model: WC_c + WPS_c + adj_c + compare_c + work_c + negate_c + Qmark_c + certain_c + reward_c + PC1 + PC2
Has 66.25291% accuracy. 

Now this mode: WC_c + WPS_c + Parenth_c + anx_c + SemiC_c + QMark_c + leisure_c + informal_c + relig_c + certain_c + PC1
Has 68.93% accuracy. 

Note: if relig_c continues to be a major predictor, perhaps use relig as a control (and check to see if I need to include interaction? (relig*relig_c) )
Notes on religion: when I have relig in the model and do the liklihood ratio test on relig_c, I get a non-significant result (ie dont add relig_c to the model). If I control for relig_c in the normal glm by using relig, I make relig_c non-significant (but accuracy drops very slightly, less than 1%). If I drop both from the model, accuracy increases to 69%. 

This model has 69.18% accuracy:
WC_c + WPS_c + Parenth_c + anx_c + SemiC_c + QMark_c + leisure_c + informal_c + certain_c+ posemo + negemo

Make sure to note in my final results that I did control for relig as well as relig*relig_c and both times accuracy suffered and the coefficient on relig_c didn't change much. 

Experimentation ends

# DONE WITH THE GOOD CODE
# DONE WITH THE GOOD CODE
# ~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~


# OLD STUFF
```{r}
ggplot(mini_df, aes(x=WC_c, y= changedview)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", fullrange=TRUE, 
              method.args = list(family=binomial)) 
```

Now let's run a logistic regression using the above variables
NOTE: this block isn't so useful anymore. 
```{r}
logit_view1 <- glm(changedview ~ WC_c, family = binomial,
                   data = mini_df)
summary(logit_view1)

```

Use this section to compare models to decide which vars to use

```{r}

glm1 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c +
              QMark_c, 
            family = binomial, 
            data = mini_df)
#summary(glm1)

glm2 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + 
              QMark_c +  certain_c,
            family = binomial,
            data = mini_df)
#summary(logit_view2)

lrtest(glm1, glm2)
```


## More robust analyses
Let's visualize the distribution of the two variables at the extremes
```{r}
ggplot(mini_df, aes(x = WC_c, y = changedview)) +
  geom_point(alpha=.1) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))

ggplot(mini_df, aes(x = QMark_c, y = changedview)) +
  geom_point(alpha=.1) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))
```

CV'd accuracy here for the entire df is 54.13%.

NOTE: USE THIS MODEL BELOW FOR YOUR WORK:
Now do crossvalidation and only use select columns. 

```{r}
# train the model on training set
set.seed(222)
#library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
mini_df$changedview <- as.factor(mini_df$changedview)
model <- train(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + home + Tone + certain_c, # WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + home + Tone + certain_c
               data = mini_df,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```

With an accuracy of 65.65%, this is the best model to use.


Note: Don't use this one. 
Creating a GLM from training data (not crossvaldiated) and testing it:

```{r}
set.seed(333)
test_obs              <- round(0.06 * nrow(df))
train_obs             <- nrow(df) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df), replace = FALSE)
test_data             <- df[which(test_train_vec == "test"),]
train_data            <- df[which(test_train_vec == "train"),]
train_data <- train_data %>% select(-c(text, area))

# create the model
glm_model      <- glm(changedview ~  PC1 + WC_c + WPS_c + adj_c + work_c + negate_c  + QMark_c + certain_c + reward_c,
                     family = binomial,
                     data = sdn)

# make the predictions
# logistic_predict      <- predict(glm_model,
#                                  test_data,
#                                  type = "response")
# 
# # show the results
# class_predictions     <- as.numeric(logistic_predict > 0.5)
# logistic_accuracy     <- mean(class_predictions == test_data$changedview)
# print(logistic_accuracy)
summary(glm_model)
```
Accuracy^

Note: Don't use this one. 
Now trying that crossvalidated and using all columns:

```{r}
#library(caret)
# define training control
#train_control <- trainControl(method = "cv", number = 10)
#mini_df$changedview <- as.factor(mini_df$changedview)
#df_g <- df_g %>% select(-post)
# train the model on training set
model <- train(changedview ~ .,
               data = sdn,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```
When controlling for the PCs, accuracy is 58.31%. 
## LASSO Logistic Regression

Creating a LASSO logistic regression and testing it (takes about 40 seconds to run)
```{r}
#df_g <- df %>% select(-c(text, area, post))
df_g <- sdn #%>% select(changedview:OtherP_c, PC1, PC2) when I filter out most poster's LIWC vars, I only get the intercept in the model
#head(df_g)
set.seed(333)
test_obs              <- round(0.03 * nrow(df_g))
train_obs             <- nrow(df_g) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df_g), replace = FALSE)
test_data             <- df_g[which(test_train_vec == "test"),]
train_data            <- df_g[which(test_train_vec == "train"),]
#train_data <- train_data %>% select(-c(text, area))
train_data_x <- df_g[ , -1]
train_data_x <- as.matrix(sapply(train_data_x, as.numeric))
train_data_y <- df_g[ , 1]
train_data_y <- as.matrix(sapply(train_data_y, as.numeric))


# create the model
glmmod <- cv.glmnet(train_data_x, train_data_y, alpha=1, 
                    nfolds = 10,
                    family="binomial",
                    type.measure = 'class')
summary(glmmod)
print("END OF MODEL SUMMARY")

test_data_x <- test_data[ , -1]
test_data_x <- as.matrix(sapply(test_data_x, as.numeric))
test_data_y <- test_data[ , 1]


# make the predictions
#logistic_predict      <- predict(glmmod,
#                                 test_data_x,
#                                 type = "response")
# show the results
#class_predictions     <- as.numeric(logistic_predict > 0.5)
#logistic_accuracy     <- mean(class_predictions == test_data$changedview)
#print(logistic_accuracy)

# let's take a look at the accuracy of this model
print("MODEL ACCURACY")
print(round(100 * mean(glmmod$cvm), 3))

# USE COEFPLOT TO PRINT OUT THE COEFFICIENTS
extract.coef(glmmod)
```

With accuracy of 37.645%, this doesn't look like a very accurate model, I'm not going to pursue it further.
Now let's take a look at it's coefficients:
```{r}

```
## END OF LASSO

## RANDOM FORESTS

Playing around with random forests
```{r}
set.seed(222)
high_sales          <- as.factor(df_g$changedview)
carseat_data        <- data.frame(df_g, high_sales)
carseat_data = carseat_data[, -1]
train               <- sample(seq(nrow(carseat_data)),
                              round(nrow(carseat_data) * 0.85))
train               <- sort(train)
test                <- which(!(seq(nrow(carseat_data)) %in% train))
carseats_tree       <- tree(high_sales ~., 
                            data = carseat_data[train,])

plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
```

Now let's get the error info:

```{r}
error_rate_func   <- function(predictions, true_vals) {
  error_rate      <- mean(as.numeric(predictions != true_vals))
  return(error_rate)
}
print("~")
deep_tree_preds   <- predict(carseats_tree,
                             carseat_data[test, ],
                             type = "class")
print("ERROR RATE IS:")
error_rate_func(deep_tree_preds, carseat_data[test,"high_sales"])
print("~")
summary(carseats_tree)
```

Misclassification error rate: 16.34%. Accuracy is: 83.66% (?)

Pruning:

```{r}
set.seed(20)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN=prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree

## Size tells us the number of terminal nodes on each
## of the trees considered; dev gives us the CV errors;
## k gives us the cost-complexity parameter.
## We can plot the error as a function of size and k

par(mfrow =c(1,2))

plot(cv_carseats_tree$size ,cv_carseats_tree$dev ,type="b")
plot(cv_carseats_tree$k ,cv_carseats_tree$dev ,type="b")

opt_indx          <- which.min(cv_carseats_tree$dev)
opt_size          <- cv_carseats_tree$size[opt_indx]

print(opt_size)

## Now we can prune the tree using prune.misclass()

pruned_carseats_tree  <- prune.misclass(carseats_tree,
                                        best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
print("ERROR RATE FUNCTION")
error_rate_func(predict(pruned_carseats_tree, carseat_data[test, ], 
                        type = "class")
                , carseat_data[test,"high_sales"])
```

Error rate function: 36.49%



```{r}
```
