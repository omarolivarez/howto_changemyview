---
title: "01 How To changemyview"
output: pdf_document
---

Steps to run before this
1. Download Google Sheet
2. Run through LIWC and save as xlsx
3. Clean up first 3 col names
4. Make sure file keeps same name (liwc_results_changemyview.xlsx) and filepath. 

Then you can start from here

```{r}
# READ IN LIBRARIES
library(data.table)
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(tidyverse)
library(stringr) # used to replace strings " of "
library(ggpubr) # used to stitch multiple graphs together
library(readxl) # to read in excel
library(corrplot) # to draw correlogram
library(corrr) # to focus correlation specifically on changedview
library(lmtest) # for likelihood test for glm
library(tree)
library(glmnet)
#library(randomForest)
#library (gbm)
```


```{r}
df_file_path <- "/Users/oolivarez/Desktop/workspaces/howto_changemyview/data/liwc_results_changemyview.xlsx"

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# and read in main, LIWCed data
df <- read_excel(df_file_path)

View(head(df, 30))
dim(df)
#head(df)
```

Need to convert changedview from character to numeric in order to use correlations and other calculations. 
```{r}
df$changedview <- as.numeric(df$changedview)
```


Let's select only relevant vars and see their correlation coefficient
```{r}
mini_df <- df %>%
  select(-c(area, text, post))
mini_df <- mini_df %>% mutate_if(is.character,as.numeric)

#mini_df = lapply(mini_df,as.numeric)
mini_df <- as.data.frame(mini_df)
#str(mini_df)

#corr_df <- mini_df %>% correlate() %>% focus(mini_df$changedview) 
#corr_df <- corr_df %>% 
#  arrange(desc(corr_df$changedview))
#View(corr_df)

small <- cor(mini_df[-1], mini_df$changedview)
small <- as.data.frame(small)
corr_df <- small %>% 
  arrange(desc(small$V1))
View(corr_df)
```

Words highly positively correlated with changing view (above 0.1 corr):
compare
WPS (is close)

Words highly negatively correlated with changing view (below -0.2 corr):
QMark
ingest
certain (is close)

## More robust analyses
Let's visualize the distribution of the two variables at the extremes
```{r}
ggplot(mini_df, aes(x = WC_c, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))

ggplot(mini_df, aes(x = Qmark_c, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))
```

Now let's run a logistic regression using the above variables
NOTE: this block isn't so useful anymore. 
```{r}
logit_view1 <- glm(changedview ~ work, family = binomial,
                   data = df)
summary(logit_view1)

```





Use this section to compare models to decide which vars to use

```{r}

glm1 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c +
              Qmark_c + home + Tone + certain_c, 
            family = binomial, 
            data = mini_df)
#summary(glm1)

glm2 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + 
              Qmark_c + home + Tone + certain_c,
            family = binomial,
            data = mini_df)
#summary(logit_view2)

lrtest(glm1, glm2)
```

Experimentation with simpler table
```{r}
sh_df <- mini_df %>% select(changedview:OtherP_c)
sh_df$changedview <- as.numeric(sh_df$changedview)

#small <- cor(sh_df[-1], sh_df$changedview)
#small <- as.data.frame(small)
#corr_df <- small %>% 
#  arrange(desc(small$V1))
#View(corr_df)

# gm1 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c +
#              Qmark_c + certain_c + reward_c, 
#             family = binomial, 
#             data = mini_df)
# #summary(glm1)
# 
# gm2 <- glm(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + 
#              Qmark_c + certain_c + reward_c + Tone_c,
#             family = binomial,
#             data = mini_df)
# #summary(logit_view2)
# 
# lrtest(gm1, gm2)

train_control <- trainControl(method = "cv", number = 10)
sh_df$changedview <- as.factor(sh_df$changedview)
model_2 <- train(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + certain_c + reward_c, 
               data = sh_df,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model_2)
print("MODEL ACCURACY")
#print(model$MAE)
print(model_2)
```
64.32% accuracy.
Experimentation ends



```{r}
ggplot(mini_df, aes(x=WC_c, y= changedview)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", fullrange=TRUE, 
              method.args = list(family=binomial)) 
```


CV'd accuracy here for the entire df is 54.13%.

NOTE: USE THIS MODEL BELOW FOR YOUR WORK:
Now do crossvalidation and only use select columns. 

```{r}
# train the model on training set
set.seed(222)
library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
mini_df$changedview <- as.factor(mini_df$changedview)
model <- train(changedview ~ WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + home + Tone + certain_c, # WC_c + WPS_c + adj_c + work_c + negate_c + Qmark_c + home + Tone + certain_c
               data = mini_df,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```

With an accuracy of 65.65%, this is the best model to use.


Note: Don't use this one. 
Creating a GLM from training data (not crossvaldiated) and testing it:

```{r}
set.seed(333)
test_obs              <- round(0.06 * nrow(df))
train_obs             <- nrow(df) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df), replace = FALSE)
test_data             <- df[which(test_train_vec == "test"),]
train_data            <- df[which(test_train_vec == "train"),]
train_data <- train_data %>% select(-c(text, area))

# create the model
glm_model      <- glm(changedview ~  compare + affiliation + QMark + leisure + certain,
                     family = binomial,
                     data = train_data)

# make the predictions
logistic_predict      <- predict(glm_model,
                                 test_data,
                                 type = "response")

# show the results
class_predictions     <- as.numeric(logistic_predict > 0.5)
logistic_accuracy     <- mean(class_predictions == test_data$changedview)
print(logistic_accuracy)
```
Accuracy^

Note: Don't use this one. 
Now trying that crossvalidated and using all columns:

```{r}
library(caret)
# define training control
train_control <- trainControl(method = "cv", number = 10)
mini_df$changedview <- as.factor(mini_df$changedview)
#df_g <- df_g %>% select(-post)
# train the model on training set
model <- train(changedview ~ .,
               data = mini_df,
               trControl = train_control,
               method = "glm",
               family=binomial())

# print cv scores
print("MODEL SUMMARY")
summary(model)
print("MODEL ACCURACY")
#print(model$MAE)
print(model)
```

## LASSO Logistic Regression

Creating a LASSO logistic regression and testing it

```{r}
#df_g <- df %>% select(-c(text, area, post))
df_g <- mini_df
#head(df_g)
set.seed(333)
test_obs              <- round(0.03 * nrow(df_g))
train_obs             <- nrow(df_g) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df_g), replace = FALSE)
test_data             <- df_g[which(test_train_vec == "test"),]
train_data            <- df_g[which(test_train_vec == "train"),]
#train_data <- train_data %>% select(-c(text, area))
train_data_x <- df_g[ , -1]
train_data_x <- as.matrix(sapply(train_data_x, as.numeric))
train_data_y <- df_g[ , 1]
train_data_y <- as.matrix(sapply(train_data_y, as.numeric))


# create the model
glmmod <- cv.glmnet(train_data_x, train_data_y, alpha=1, 
                    nfolds = 10,
                    family="binomial",
                    type.measure = 'class')
summary(glmmod)
print("END OF MODEL SUMMARY")

test_data_x <- test_data[ , -1]
test_data_x <- as.matrix(sapply(test_data_x, as.numeric))
test_data_y <- test_data[ , 1]


# make the predictions
#logistic_predict      <- predict(glmmod,
#                                 test_data_x,
#                                 type = "response")
# show the results
#class_predictions     <- as.numeric(logistic_predict > 0.5)
#logistic_accuracy     <- mean(class_predictions == test_data$changedview)
#print(logistic_accuracy)

# let's take a look at the accuracy of this model
print("MODEL ACCURACY")
print(round(100 * mean(glmmod$cvm), 3))
```

With accuracy of 37.645%, this doesn't look like a very accurate model, I'm not going to pursue it further.


## RANDOM FORESTS

Playing around with random forests
```{r}
set.seed(222)
high_sales          <- as.factor(df_g$changedview)
carseat_data        <- data.frame(df_g, high_sales)
carseat_data = carseat_data[, -1]
train               <- sample(seq(nrow(carseat_data)),
                              round(nrow(carseat_data) * 0.85))
train               <- sort(train)
test                <- which(!(seq(nrow(carseat_data)) %in% train))
carseats_tree       <- tree(high_sales ~., 
                            data = carseat_data[train,])

plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
```

Now let's get the error info:

```{r}
error_rate_func   <- function(predictions, true_vals) {
  error_rate      <- mean(as.numeric(predictions != true_vals))
  return(error_rate)
}
print("~")
deep_tree_preds   <- predict(carseats_tree,
                             carseat_data[test, ],
                             type = "class")
print("ERROR RATE IS:")
error_rate_func(deep_tree_preds, carseat_data[test,"high_sales"])
print("~")
summary(carseats_tree)
```

Misclassification error rate: 16.34%. Accuracy is: 83.66% (?)

Pruning:

```{r}
set.seed(20)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN=prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree

## Size tells us the number of terminal nodes on each
## of the trees considered; dev gives us the CV errors;
## k gives us the cost-complexity parameter.
## We can plot the error as a function of size and k

par(mfrow =c(1,2))

plot(cv_carseats_tree$size ,cv_carseats_tree$dev ,type="b")
plot(cv_carseats_tree$k ,cv_carseats_tree$dev ,type="b")

opt_indx          <- which.min(cv_carseats_tree$dev)
opt_size          <- cv_carseats_tree$size[opt_indx]

print(opt_size)

## Now we can prune the tree using prune.misclass()

pruned_carseats_tree  <- prune.misclass(carseats_tree,
                                        best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)
print("ERROR RATE FUNCTION")
error_rate_func(predict(pruned_carseats_tree, carseat_data[test, ], 
                        type = "class")
                , carseat_data[test,"high_sales"])
```

Error rate function: 36.49%



```{r}
```
