---
title: "01 How To changemyview"
output: pdf_document
---

Steps to run before this
1. Download Google Sheet
2. Run through LIWC and save as xlsx
3. Clean up first 3 col names
4. Make sure file keeps same name (liwc_results_changemyview.xlsx) and filepath. 

Then you can start from here

```{r}
# READ IN LIBRARIES
library(data.table)
library(readr)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lubridate)
library(tidyverse)
library(stringr) # used to replace strings " of "
library(ggpubr) # used to stitch multiple graphs together
library(readxl) # to read in excel
library(corrplot) # to draw correlogram
library(corrr) # to focus correlation specifically on changedview
library(lmtest) # for likelihood test for glm
#library(randomForest)
#library (gbm)
```


```{r}
df_file_path <- "/Users/OmarOlivarez/Desktop/workspaces/howto_changemyview/data/liwc_results_changemyview.xlsx"

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# and read in main, LIWCed data
df <- read_excel(df_file_path)

View(head(df, 30))
#head(df)
```

Need to convert changedview from character to numeric in order to use correlations and other calculations. 
```{r}
df$changedview <- as.numeric(df$changedview)
```


Let's select only relevant vars and see their correlation coefficient
```{r}
mini_df <- df %>%
  select(-c(area, text, WC))

corr_df <- mini_df %>% correlate() %>% focus(changedview) 

corr_df <- corr_df %>% 
  arrange(desc(corr_df$changedview))
View(corr_df)
```

Words highly positively correlated with changing view (above 0.1 corr):
compare
WPS (is close)

Words highly negatively correlated with changing view (below -0.2 corr):
QMark
ingest
certain (is close)

## More robust analyses
Let's visualize the distribution of the two variables at the extremes
```{r}
ggplot(df, aes(x = compare, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))

ggplot(df, aes(x = QMark, y = changedview)) +
  geom_point(alpha=.5) + 
  theme(panel.background = element_rect(fill = "white", colour= "black")) +
  theme(panel.grid.major = element_line(linetype = "dashed", color = "gray"))
```

Now let's run a logistic regression using the above variables

```{r}
logit_view1 <- glm(changedview ~ compare, family = binomial,
                   data = df)
summary(logit_view1)

logit_view2 <- glm(changedview ~ compare + QMark, 
                   family = binomial,
                   data = df)
summary(logit_view2)

logit_view4 <- glm(changedview ~ compare + affiliation + QMark + leisure + certain, 
                   family=binomial,
                   data = df)

summary(logit_view4)
```





Use this section to compare models to decide which vars to use

```{r}

glm1 <- glm(changedview ~ compare + affiliation +
              QMark + leisure + certain, 
            family = binomial, 
            data = df)
#summary(glm1)

glm2 <- glm(changedview ~ compare + affiliation + 
              QMark + leisure + certain + auxverb,
            family = binomial,
            data = df)
#summary(logit_view2)

lrtest(glm1, glm2)
```










Let's run a likelihood ratio test on model3 vs model1
```{r}
lrtest(logit_view1, logit_view4)

```




```{r}
ggplot(df, aes(x=compare, y= changedview)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", fullrange=TRUE, 
              method.args = list(family=binomial)) 
```


Just playing around a bit:

```{r}
set.seed(222)
test_obs              <- round(0.03 * nrow(df))
train_obs             <- nrow(df) - test_obs
test_train_vec        <- c(rep("test", test_obs),
                           rep("train", train_obs))

test_train_vec        <- sample(test_train_vec, nrow(df), replace = FALSE)
test_data             <- df[which(test_train_vec == "test"),]
train_data            <- df[which(test_train_vec == "train"),]
train_data <- train_data %>% select(-c(text, area))

# create the model
glm_model      <- glm(changedview ~  compare + affiliation + QMark + leisure + certain,
                     family = binomial,
                     data = train_data)

# make the predictions
logistic_predict      <- predict(glm_model,
                                 test_data,
                                 type = "response")

# show the results
class_predictions     <- as.numeric(logistic_predict > 0.5)
logistic_accuracy     <- mean(class_predictions == test_data$changedview)
print(logistic_accuracy)
```
Accuracy^







Playing around with random forests
```{r}
set.seed(222)
high_sales          <- as.factor(df$changedview)
carseat_data        <- data.frame(df, high_sales)
carseat_data = carseat_data[, -1]
train               <- sample(seq(nrow(carseat_data)),
                              round(nrow(carseat_data) * 0.85))
train               <- sort(train)
test                <- which(!(seq(nrow(carseat_data)) %in% train))
carseats_tree       <- tree(high_sales ~., 
                            data = carseat_data[train,])

plot(carseats_tree)
text(carseats_tree, pretty = 0)
carseats_tree
```

Now let's get the error info:

```{r}
error_rate_func   <- function(predictions, true_vals) {
  error_rate      <- mean(as.numeric(predictions != true_vals))
  return(error_rate)
}
print("~")
deep_tree_preds   <- predict(carseats_tree,
                             carseat_data[test, ],
                             type = "class")
print("ERROR RATE IS:")
error_rate_func(deep_tree_preds, carseat_data[test,"high_sales"])
print("~")
summary(carseats_tree)
```


Pruning:

```{r}
set.seed(20)
cv_carseats_tree  <- cv.tree(carseats_tree, FUN=prune.misclass)
names(cv_carseats_tree)
cv_carseats_tree

## Size tells us the number of terminal nodes on each
## of the trees considered; dev gives us the CV errors;
## k gives us the cost-complexity parameter.
## We can plot the error as a function of size and k

par(mfrow =c(1,2))

plot(cv_carseats_tree$size ,cv_carseats_tree$dev ,type="b")
plot(cv_carseats_tree$k ,cv_carseats_tree$dev ,type="b")

opt_indx          <- which.min(cv_carseats_tree$dev)
opt_size          <- cv_carseats_tree$size[opt_indx]

print(opt_size)

## Now we can prune the tree using prune.misclass()

pruned_carseats_tree  <- prune.misclass(carseats_tree,
                                        best = opt_size)
plot(pruned_carseats_tree)
text(pruned_carseats_tree, pretty = 0)

error_rate_func(predict(pruned_carseats_tree, carseat_data[test, ], 
                        type = "class")
                , carseat_data[test,"high_sales"])
```





```{r}
```
